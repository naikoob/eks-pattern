:icons: font
:imagesdir: ./images
:source-highlighter: coderay
:toc: left

= Highly available application on EKS

This one stop document is a collection (of links) of best practices to develop and deploy highly available (HA) applications on Kubernetes, specifically https://aws.amazon.com/eks/[Amazon Elastic Kubernetes Service (EKS)]. It is not about building a highly available kubernetes cluster (multiple master, etcd, etc), because that's taken care of by EKS.

== Applications, deployments, services

The basic premise of high availability is to eliminate single points of failure. In Kubernetes, this means there should be multiple replicas of your application. Applications should always be deployed using a k8s https://kubernetes.io/docs/concepts/workloads/controllers/deployment/[deployment] instead of pods (even when you only need a single replica), and exposed to consumers as a https://kubernetes.io/docs/concepts/services-networking/service/[service]. However, that's just the beginning...

=== Health checks

In order for Kubernetes to replace failed pods, it needs a good indication of when a pod has failed. By default, kubernetes deem a container failed when the main process (the `ENTRYPOINT` and/or `CMD` of your `Dockerfile`) terminates. This is fine for some applications. However, for request-response workload such as a REST or gRPC endpoint, there may be situation when process is still active, but no longer responsive to requests. For such workload, we can improve this with https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/[liveness and readiness probes]. 

Liveness probes provide a way for k8s to know if the application is healthy. Readiness probe provide a way for k8s to know if a newly started pod is ready to service requests. This can be helpful for application that has slow startup time. 

Used appropriately, liveness and readiness probes can improve resilency.

Most modern web frameworks provides capability to easily inject a health check into your application (e.g. https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-features.html[Spring Boot Actuator]).

=== Graceful termination

Care can also be exercised to improve robustness when application terminates. When a https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods[pod is terminated], a `TERM` signal is sent to the main process in each container. After a grace period has expired, the `KILL` signal is sent to those processes. In addition, kubernetes also provides https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/[lifecycle hooks] to provide finer control of the pod shutdown process.

Depending on the programming language/application framework used, it may be necessary to handle the `TERM` signal (or implement a `preStop` hook) to release/cleanup any shared resources (e.g. database connection, file locks, etc.). 

If the pod is for some reason directly attached to a load balancer, this is also the opportunity to unregister itself from the load balancer to stop sending new requests during the shutdown process (and other reason to expose your application as a k8s service).

=== Horizontal pod autoscaler

It is often desireable to be able to scale application automatically. In k8s, the model is to scale horizontally (adding more pods). The mechanism is aptly called https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/[horizontal pod autoscaler]. Example:
[source,yaml]
----
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu <1>
      target:
        type: Utilization
        averageUtilization: 50
----

<1> in this example we're scaling based on cpu utilization

IMPORTANT: For hpa to work, it must be able to access metrics to determine when to scale in/out. 
EKS does not enable metrics by default. You need to https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html[install k8s metrics server on your cluster]. In addition, you can https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html[use Prometheus to provide custom metrics] as input to autoscaling.

=== Affinity and anti-affinity

For HA, we want to distribute replicas of the same deployment across 2 or more nodes. K8s uses a system of labels and selectors to determine which node to schedule pods. In addition, we can use https://v1-14.docs.kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity[affinity and anti-affinity] declaration to influence where your pods get scheduled.

Example: 
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store <1>
    spec:
      affinity:
        podAntiAffinity: <2>
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:3.2-alpine
----

<1> Deployment has a label `app=store`
<2> Anti affinity declaration to avoid being place alongside pods with label `app=store`

NOTE: Sometime it's desirable to place related deployments on the same nodes for better performance. E.g. https://eksworkshop.com/beginner/140_assigning_pods/affinity_usecases/[a web server and a redis cache]

NOTE: In addition to label/selector and affinity/anti-affinity, k8s also provides https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/[taints and tolerations] to allow a node to repel a set of pods.

== Nodes, node groups

In EKS, nodes (groups) are provisioned as https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html[EC2 Auto Scaling Groups]. Check https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html[here] on how to launch worker nodes.

NOTE: Beginning with EKS 1.14, AWS launched https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html[Managed Node Groups] to make it easier to provision and manage worker nodes.

For high availability, nodes should be spread across 2 or more availablity zones. This can be achieved by a single node group spaning multiple AZ or dedicated node group for each AZ.

=== Cluster autoscaler

https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler[Cluster autoscaler] automatically adjusts the number of nodes in a Kubernetes cluster. 

NOTE: Cluster autoscaler is not setup by default, the documentation to set it up on EKS can be found https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html[here], and there's also an article from knowledge center https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/[here].

==== Over provisioning 

While cluster autoscaler dynamically adjust the number of nodes in a cluster, it takes time to spin up a new node and have it join the cluster. We can make use of low priority deployments to over provision worker nodes. This process is described in the cluster-autoscaler project https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler[here]. A https://hub.helm.sh/charts/stable/cluster-overprovisioner[helm chart] is also available. There are also blogs https://tech.deliveryhero.com/dynamically-overscaling-a-kubernetes-cluster-with-cluster-autoscaler-and-pod-priority/[here] and https://medium.com/scout24-engineering/cluster-overprovisiong-in-kubernetes-79433cb3ed0e[here].

=== Replacing nodes

Every now and then we'll need to update our worker nodes, like applying patches, or upgrading the Kubernetes component version. In cloud native spirit, we replace nodes with new patches/version installed instead of apply changes to nodes in-place. This means spinning up new nodes or node groups and drain pods from the old nodes to the new nodes. This process is https://docs.aws.amazon.com/eks/latest/userguide/update-workers.html[document here] for self managed EKS nodes. For managed node groups, please refer to https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html[documentation here].

=== Disruption budget

As pods are scheduled dynamically across nodes, there may be risks of evacuating too many pods of the same application during the draining process. We use https://kubernetes.io/docs/tasks/run-application/configure-pdb/[PodDisruptionBudget] to minimise this. PodDisruptionBudget is a k8s construct that let us specify the min/max available/unavailable tolerance for a deployment. 

For example:
[source,yaml]
----
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  minAvailable: 2 <1>
  selector:
    matchLabels:
      app: zookeeper
----

<1> minimum 2 copies of pods with label `app=zookeeper` should be running

Check https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work[here for how disruption budgets work].

== Observability

Oberservability is achieved when the data is made available from within the system that you wish to monitor. These data includes logs and metrics.

=== Logs

==== Control plane logs

EKS does not enable cluster control plane logs by default (because there's https://aws.amazon.com/cloudwatch/pricing/[cost] involved). For production clusters, it is important to enable these logs. Control plane logs can be enabled from the AWS Console, CLI or APIs, as described https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html[here].

==== Application logs

When running containers at scale, especially when adopting a microservice approach, it is important to have a logging infrastucture to aggregate logs from different deployments.

In k8s community, the most common solution is the EFK stack. Here's a https://eksworkshop.com/intermediate/230_logging/[guide on EFK at eksworkshop.com].

For an AWS based solution, we can also https://aws.amazon.com/blogs/opensource/centralized-container-logging-fluent-bit/[ship logs to S3 via Kinesis Firehose and query using Athena].

=== Metrics
==== Container insights

TODO

=== Tracing

TODO

== Multi-cluster

In some cases, it may be desireable to deploy applications across multiple clusters. For example, to serve different geographical regions or just to have higher resilency at control plane level. For that, we can make use of Route53 to distribute requests to multiple clusters, as depicted below:

image::multiple-clusters.png[]

NOTE: As most applications have external dependencies, such as a persistence backend, these dependencies should be available to both clusters for the above topology to work.

=== Federation

TODO

== Infrastructure as code

TODO

== References

. https://eksworkshop.com[EKS Workshop]
. https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html[Installing the Kubernetes Metrics Server on EKS]

TODO
